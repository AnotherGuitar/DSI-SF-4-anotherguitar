{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pip install scrapy\n",
    "# pip install --upgrade zope2\n",
    "\n",
    "from collections import Counter\n",
    "import foursquare\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "from scrapy import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "import seaborn as sns\n",
    "import textacy\n",
    "import textacy.data\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CLIENT_ID = '33NDJLQ342FAMTNX5Z55PR0PQQOJZRAZZ3XEAI0ERQXEJRUL'\n",
    "CLIENT_SECRET = 'FGMFNZGMWUR1ILZFGH2NV1OKQY3WK5AAPHWKXTFWRR3B4Z4E'\n",
    "client = foursquare.Foursquare(client_id=CLIENT_ID, client_secret=CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-122.355079651\n",
      "[37.8127675576, 37.7078622611]\n",
      "360\n"
     ]
    }
   ],
   "source": [
    "#Let's define a geo dictionary whose bounds encompass all of SF, and another 4 miles south as well (so ~7mi x 11mi)\n",
    "ne = {'ne_lat': -122.3550796509, 'ne_long': 37.8127675576}\n",
    "sw = {'sw_lat': -122.5164413452, 'sw_long': 37.7078622611}\n",
    "\n",
    "# east/west\n",
    "lat_bounds = [ne['ne_lat'], sw['sw_lat']]\n",
    "print lat_bounds[0]\n",
    "# north/south\n",
    "lon_bounds = [ne['ne_long'], sw['sw_long']]\n",
    "print lon_bounds\n",
    "\n",
    "#increment ~ half a mile (in latitude/longitude), is 0.007\n",
    "increment = 0.007\n",
    "\n",
    "# The gridding below moves North, starting from the bottom SW corner boundary, and then moves east half a mile,\n",
    "# and repeats the process until stopping at the NE corner boundary.\n",
    "grid_pairs = []\n",
    "for lat in np.arange(lat_bounds[1], lat_bounds[0], increment):\n",
    "    for lon in np.arange(lon_bounds[1], lon_bounds[0], increment):\n",
    "        grid_pairs.append([lat, lon])\n",
    "        \n",
    "print len(grid_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTES:\n",
    "# should i make the loop above this cell more efficient by checking and storing which url's don't work?\n",
    "# B/C then I can exclude them the next time i run my loops ; BUT, it's probably better to keep track of these,\n",
    "# because while some won't even be eateries, many will be eateries that simply don't have foursqare menus.\n",
    "\n",
    "# In such cases, knowing the venue information could still be valuable, because we can surface those to users who\n",
    "# wish to manually add items (could possibly add items by taking pictures of the menu where the item is located)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here, I work with the Explore Endpoint\n",
    "\n",
    "# This will pull the venue names and url's (labeled as 'explored_venue_ids') that correspond to the geographical\n",
    "# areas I paired off in the previous step with grid_pairs. These are the  urls that I then intend to scrape in\n",
    "# subsequent steps.\n",
    "\n",
    "unique_venues_from_explore = []\n",
    "unique_venue_names_from_explore = []\n",
    "start_time = time.time()\n",
    "counter = 0\n",
    "for x, y in grid_pairs:\n",
    "    for offset in range(50, 251, 50):\n",
    "        try:\n",
    "            # Radius is radius in meters around given 'll'; 800 meters\n",
    "            # is approx 0.5 miles (but I may adjust the radius going forward)\n",
    "        \n",
    "            explore = client.venues.explore(params={'ll': '%.2f, %.2f' % (y, x), 'llAcc':'100.0','radius': '2000',\n",
    "                                               'section': 'food','limit':'50','offset':''+str(offset)+'',\n",
    "                                                    'sortByDistance':'1'})\n",
    "            explored_venue_ids = []\n",
    "            explored_venue_names = []\n",
    "\n",
    "            for i in range(len(explore['groups'][0]['items'])):\n",
    "                try:\n",
    "                    pulled_id = explore['groups'][0]['items'][i]['venue']['menu']['url']\n",
    "                    explored_venue_ids.append(pulled_id)\n",
    "                    pulled_name = explore['groups'][0]['items'][i]['venue']['name']\n",
    "                    explored_venue_names.append(pulled_name)\n",
    "                except:\n",
    "                    pass\n",
    "            for next_id, next_name in zip(explored_venue_ids, explored_venue_names):\n",
    "                if 'foursquare.com' in str(next_id):\n",
    "                    unique_venues_from_explore.append(next_id)\n",
    "                    unique_venue_names_from_explore.append(next_name)\n",
    "            counter += 1\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time)), \"loop number:\", counter\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print len(unique_venue_names_from_explore), len(unique_venues_from_explore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_venues_from_explore = list(set(unique_venues_from_explore))\n",
    "unique_venue_names_from_explore = list(set(unique_venue_names_from_explore))\n",
    "unique_but_chained_venues = len(unique_venues_from_explore) - len(unique_venue_names_from_explore)\n",
    "\n",
    "print len(unique_venues_from_explore), unique_but_chained_venues\n",
    "# So ~ 18% of our results are unique venues (and another 48 of them are unique venus,\n",
    "# but w/ the same name i.e. chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now that I have plenty of urls, let's plug them into a scraper so I can populate my df. Here are the headers\n",
    "# I'm looking to get as well...\n",
    "\n",
    "column_headers = ['venue_name', 'venue_desc_list', 'vegan_venue_check', 'venue_menu_url', 'venue_rated',\n",
    "                  'meta_menu_n', 'depth_menus_n', 'menu_item_name', 'menu_item_price', 'menu_item_desc']\n",
    "\n",
    "df_ready_rows = []\n",
    "def parse_url(url, data=False):\n",
    "\n",
    "    response               =  requests.get(url)\n",
    "    \n",
    "    #Steps:\n",
    "    #1) get the unicode objects\n",
    "    #2) change objects from unicode to string\n",
    "    \n",
    "    venue_name_uni         = Selector(text=response.text).xpath('//h1[@class=\"venueName\"]/text()').extract()\n",
    "    venue_name             = unicodedata.normalize('NFKD', venue_name_uni[0]).encode('ascii','ignore')\n",
    "    \n",
    "    # I also need to do an iteration to capture the multiple descriptors.\n",
    "    # I'll store the descriptors in a list to capture the entire description:\n",
    "    venue_desc_uni         =  Selector(text=response.text).xpath('//span[@class=\"unlinkedCategory\"]/\\\n",
    "    text()').extract()\n",
    "    venue_desc_list        = []\n",
    "    vegan_venue_check      = [] # I'll also do a quick check to see up front if we have a vegan menu on our hands\n",
    "    for venue_desc_phrase in range(len(venue_desc_uni)):\n",
    "        venue_desc_n       = unicodedata.normalize('NFKD', venue_desc_uni[venue_desc_phrase]).encode('ascii',\n",
    "                                                                                                  'ignore')\n",
    "        venue_desc_list.append(venue_desc_n)\n",
    "\n",
    "    # Here I am going to manually label all my menu items which correspond to restaurants that Foursquare already\n",
    "    # recognizes as vegan. They're specific label for a vegan venue is 'Vegetarian / Vegan Restaurant' - this may\n",
    "    # be misleading and incorrectly read as vegetarian or vegan, with it in fact means that it is vegetarian, but\n",
    "    # more specifically, vegan.\n",
    "    \n",
    "    flat_desc = ' '.join(venue_desc_list)\n",
    "    if 'vegetarian / vegan restaurant' in flat_desc.lower():\n",
    "        vegan_venue_check  = 'vegan'\n",
    "    else:\n",
    "        vegan_venue_check  = 'not_vegan'\n",
    "\n",
    "    # The url too, right?\n",
    "    venue_menu_url         = url\n",
    "\n",
    "    # Grabbing the venue rating, just a note: venueScore positive/neutral/negative, but I'm only getting the\n",
    "    # rating number, on a scale 1-10\n",
    "    venueScore_options     = ['positive','neutral','negative']\n",
    "    venue_rated = 'placeholder'\n",
    "    for vs_option in venueScore_options:\n",
    "        try:\n",
    "            venue_rating_uni        = Selector(text=response.text).xpath('//div[@class=\"venueRateBlock  \"]/\\\n",
    "    span[@class=\"venueScore '+vs_option+'\"]/span/text()').extract()\n",
    "            venue_rating            = unicodedata.normalize('NFKD', venue_rating_uni[0]).encode('ascii','ignore')\n",
    "            venue_rated             = float(venue_rating)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # Even if there is no rating, I'd still like to keep track of that...\n",
    "    if venue_rated == 'placeholder':\n",
    "        venue_rated = 'rating_not_available'\n",
    "        \n",
    "    #And I'll transform the list back into a string...\n",
    "#     venue_rated = venue_rated[0]\n",
    "    \n",
    "    #NOTE: do i also need to account for when menus don't have titles? because in that case meta_menu_list\n",
    "    #could/would\n",
    "    #return null. if so, perhaps just do a 'try excepct:pass' function if it can't find titles, but could it still\n",
    "    #grab the menu items? maybe i should just put in a \"null title\" for the meta_menu_n to overcome this\n",
    "    #I no longer think this is an issue, but maybe something to put in the appendix for later?\n",
    "    \n",
    "    meta_menu_list      =  Selector(text=response.text).xpath('//h2[@class=\"categoryName\"]/text()').extract()\n",
    "        \n",
    "    for meta_menu_item in range(len(meta_menu_list)):\n",
    "        \n",
    "        meta_menu_n         = unicodedata.normalize('NFKD', meta_menu_list[meta_menu_item]).encode('ascii',\n",
    "                                                                                                   'ignore')\n",
    "        \n",
    "#         print \"meta menu title %d:\" %(meta_menu_item+1), meta_menu_n, \"# of meta menus:\", len(meta_menu_list)\n",
    "        \n",
    "        depth_menus_n_uni = Selector(text=response.text).xpath('//div[@class=\"menu\"]['+str(meta_menu_item+1)+']/\\\n",
    "        div[@class=\"menuItems\"]/div[@class=\"section\"]/div[@class=\"sectionHeader\"]/\\\n",
    "        div[@class=\"sectionName\"]/text()').extract()\n",
    "        \n",
    "        for meta_depth_nn in range(len(depth_menus_n_uni)):\n",
    "            \n",
    "            depth_menus_n     = unicodedata.normalize('NFKD', depth_menus_n_uni[meta_depth_nn]\n",
    "                                                     ).encode('ascii','ignore')\n",
    "\n",
    "            #get the name of the depth menu, and record it's location as 'n_level'\n",
    "            n_level = meta_depth_nn+1\n",
    "#             print \"depth menu title %d:\" %(n_level), depth_menus_n\n",
    "            \n",
    "            #let's grab the entire depth menu:\n",
    "            depth_menu_id_uni = Selector(text=response.text).xpath('//div[@class=\"menu\"]\\\n",
    "            ['+str(meta_menu_item+1)+']/div[@class=\"menuItems\"]/div[@class=\"section\"]['+str(n_level)+']/\\\n",
    "            div[@class=\"sectionHeader\"]/div[@class=\"sectionName\"]/text()').extract()\n",
    "            depth_menu_id     = unicodedata.normalize('NFKD', depth_menu_id_uni[0]\n",
    "                                                     ).encode('ascii','ignore')\n",
    "            depth_menu_id = len(depth_menu_id_uni)\n",
    "#             print \"#id of depth menu:\", depth_menu_id\n",
    "            \n",
    "            #loop throught the left and right side of each container:\n",
    "            left_or_right_list = ['left','right']\n",
    "            \n",
    "            for left_or_right in left_or_right_list:\n",
    "    \n",
    "                #need the length of the [left/right] container, to iterate through:\n",
    "                container_len_uni = Selector(text=response.text).xpath('//div[@class=\"menu\"]\\\n",
    "                ['+str(meta_menu_item+1)+']/div[@class=\"menuItems\"]/div[@class=\"section\"]['+str(n_level)+']/div\\\n",
    "                [@class=\"entryContainer\"]/div[@class=\"'+left_or_right+'Column\"]/\\\n",
    "                div[@class=\"entry\"]/node()[1]//text()').extract()\n",
    "#                 print \"left_check:\", left_or_right, \"contain len:\", len(container_len_uni)\n",
    "            \n",
    "                for section_n in range(len(container_len_uni)):                    \n",
    "                    \n",
    "                    #now we can get the name of that menu item...\n",
    "                    menu_item_name_uni = Selector(text=response.text).xpath('//div[@class=\"menu\"]\\\n",
    "                    ['+str(meta_menu_item+1)+']/div[@class=\"menuItems\"]/div[@class=\"section\"]\\\n",
    "                    ['+str(n_level)+']/div\\\n",
    "                    [@class=\"entryContainer\"]/div[@class=\"'+left_or_right+'Column\"]/div[@class=\"entry\"]\\\n",
    "                    ['+str(section_n+1)+']/node()[1]//text()').extract()\n",
    "                    menu_item_name     = unicodedata.normalize('NFKD', menu_item_name_uni[0]\n",
    "                                                                    ).encode('ascii','ignore')\n",
    "#                     print \"menu_item_name:\", menu_item_name\n",
    "                    \n",
    "                    #and then we can get the price (if there is one...)\n",
    "                    try:\n",
    "                        menu_item_price_uni = Selector(text=response.text).xpath('//div[@class=\"menu\"]\\\n",
    "                    ['+str(meta_menu_item+1)+']/div[@class=\"menuItems\"]/div[@class=\"section\"]\\\n",
    "                    ['+str(n_level)+']/div\\\n",
    "                    [@class=\"entryContainer\"]/div[@class=\"'+left_or_right+'Column\"]/div[@class=\"entry\"]\\\n",
    "                    ['+str(section_n+1)+']/node()[2]//text()').extract()\n",
    "                        menu_item_price     = unicodedata.normalize('NFKD', menu_item_price_uni[0]\n",
    "                                                                    ).encode('ascii','ignore')\n",
    "                        menu_item_price = float(menu_item_price)\n",
    "#                         print \"menu_item_price:\", menu_item_price\n",
    "                    except:\n",
    "#                         print \"menu_item_price:\", \"price_not_available\"\n",
    "                        menu_item_price = 'price_not_available'\n",
    "                    \n",
    "                    #and finally the description (if there is one...)\n",
    "                    try:\n",
    "                        menu_item_desc_uni = Selector(text=response.text).xpath('//div[@class=\"menu\"]\\\n",
    "                    ['+str(meta_menu_item+1)+']/div[@class=\"menuItems\"]/div[@class=\"section\"]\\\n",
    "                    ['+str(n_level)+']/div\\\n",
    "                    [@class=\"entryContainer\"]/div[@class=\"'+left_or_right+'Column\"]/div[@class=\"entry\"]\\\n",
    "                    ['+str(section_n+1)+']/node()[3]//text()').extract()\n",
    "                        menu_item_desc     = unicodedata.normalize('NFKD', menu_item_desc_uni[0]\n",
    "                                                                    ).encode('ascii','ignore')\n",
    "#                         print \"menu_item_desc:\", menu_item_desc\n",
    "                    except:\n",
    "#                         print \"menu_item_desc:\", \"desc_not_available\"\n",
    "                        menu_item_desc = 'desc_not_available'\n",
    "\n",
    "                    # Finally, I'll append my results so that when I wrap up the fuction, I can finish with\n",
    "                    # a prepared set of info, dataframe ready.\n",
    "                    df_ready_rows.append([venue_name,\n",
    "                                          venue_desc_list,\n",
    "                                          vegan_venue_check,\n",
    "                                          venue_menu_url,\n",
    "                                          venue_rated,\n",
    "                                          meta_menu_n,\n",
    "                                          depth_menus_n,\n",
    "                                          menu_item_name,\n",
    "                                          menu_item_price,\n",
    "                                          menu_item_desc])\n",
    "    return df_ready_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Actually, through the Explore endpoint, I was able to directly grab the menu url, so no need to manually build my\n",
    "# url this time...\n",
    "start_time = time.time()\n",
    "countered = 0\n",
    "for menu_url in unique_venues_from_explore:\n",
    "    try:\n",
    "        parse_url(menu_url)\n",
    "    except:\n",
    "        pass\n",
    "    countered += 1\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time)), \"loop number:\", countered\n",
    "# Takes about 4 mins for 30 url's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I ran my script previously, and just saved a local copy...\n",
    "\n",
    "explore_endpoint_df = pd.DataFrame(df_ready_rows, columns=column_headers)\n",
    "explore_endpoint_df.shape\n",
    "\n",
    "explore_endpoint_df.to_pickle('../../projects/Capstone Stuff/explore_endpoint_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food = pd.read_pickle('../../projects/Capstone Stuff/explore_endpoint_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90047, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "not_vegan    88186\n",
       "vegan         1861\n",
       "Name: vegan_venue_check, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food.vegan_venue_check.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "food_desc = food[food.menu_item_desc != 'desc_not_available']\n",
    "# food_desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I start playing with textacy here below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 14 lbs. super sweet, its the crab youll love to eat!'\n",
      " 'Burrata imported weekly from the southern Italian region of Puglia, roasted butternut squash'\n",
      " '12 oz regular or fat free'\n",
      " 'Langostino, avocado, cucumber wrapped with soy paper and aioli and topped with salmon.'\n",
      " 'Tangerine mousse, basil, coconut tuile; Suggested pairing: Moscatel Malaga, Jorge Odonez #2 2012']\n"
     ]
    }
   ],
   "source": [
    "texts = food_desc.sample(n=1000).menu_item_desc.values\n",
    "print texts[0:5]\n",
    "docs = textacy.corpus.Corpus(u'en', texts=(x.decode('utf-8','ignore') for x in\n",
    "                                           food_desc.sample(n=1000).menu_item_desc.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sauteed lobster roll, braised filet mignon, shiitake mushrooms, wasabi mascarpone, side]\n",
      "[]\n",
      "[chocolate sponge cake, dark chocolate mousse, white chocolate mousse, tall sheet, white chocolate, top, cake, fresh seasonal fruit]\n",
      "[Boiled potatoes, parsley, bell pepper, red onions, celery, light dijonnaise, garnish]\n",
      "[Oregano, garlic]\n",
      "[delicious blend, miso, green onion, tofu]\n",
      "[Sauteed clams, shrimps, squid, fresh chili, garlic, basil leaves, onion, hot plate]\n",
      "[Sweet onion, roasted chile-garlic, yogurt mint, tamarind, pappadum]\n",
      "[Ice cream sandwich, olive oil ice cream]\n",
      "[Black garlic]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print [x for x in textacy.extract.noun_chunks(docs[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some tom-foolery w/ regards to the explore_endpoint_df is below, before my final notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here I'm just testing out a prototype search function\n",
    "count = 0\n",
    "loc_list = []\n",
    "for i in range(food.shape[0]):\n",
    "    flat_desc = ' '.join(food.venue_desc_list[i])\n",
    "    if 'american' in flat_desc.lower():\n",
    "        count += 1\n",
    "        loc_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>venue_name</th>\n",
       "      <th>venue_desc_list</th>\n",
       "      <th>vegan_venue_check</th>\n",
       "      <th>venue_menu_url</th>\n",
       "      <th>venue_rated</th>\n",
       "      <th>meta_menu_n</th>\n",
       "      <th>depth_menus_n</th>\n",
       "      <th>menu_item_name</th>\n",
       "      <th>menu_item_price</th>\n",
       "      <th>menu_item_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88481</th>\n",
       "      <td>Johnny Rockets</td>\n",
       "      <td>[American Restaurant, Burger Joint]</td>\n",
       "      <td>not_vegan</td>\n",
       "      <td>https://foursquare.com/v/johnny-rockets/4ba663...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>Main Menu</td>\n",
       "      <td>Desserts</td>\n",
       "      <td>Perfect Brownie Sundae</td>\n",
       "      <td>price_not_available</td>\n",
       "      <td>A decadent blend of gourmet chocolate goodness...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           venue_name                      venue_desc_list vegan_venue_check  \\\n",
       "88481  Johnny Rockets  [American Restaurant, Burger Joint]         not_vegan   \n",
       "\n",
       "                                          venue_menu_url venue_rated  \\\n",
       "88481  https://foursquare.com/v/johnny-rockets/4ba663...         7.3   \n",
       "\n",
       "      meta_menu_n depth_menus_n          menu_item_name      menu_item_price  \\\n",
       "88481   Main Menu      Desserts  Perfect Brownie Sundae  price_not_available   \n",
       "\n",
       "                                          menu_item_desc  \n",
       "88481  A decadent blend of gourmet chocolate goodness...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in loc_list:\n",
    "    dffff = food.loc[[i]]\n",
    "dffff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/dsi/lib/python2.7/site-packages/ipykernel/__main__.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "970"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(food[food.vegan_venue_check == 'vegan'][\n",
    "    food.menu_item_desc != 'desc_not_available'].menu_item_desc.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/dsi/lib/python2.7/site-packages/ipykernel/__main__.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  if __name__ == '__main__':\n",
      "/anaconda/envs/dsi/lib/python2.7/site-packages/ipykernel/__main__.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "blah = food[food.vegan_venue_check == 'vegan'][food.menu_item_name != 'desc_not_available'][\n",
    "    food.menu_item_desc != 'desc_not_available'][['venue_name','depth_menus_n','menu_item_name','menu_item_desc']\n",
    "                                                ].reset_index()\n",
    "blah.drop('index', axis=1, inplace=True)\n",
    "# Here we have vegan food items, where we have item names and descriptions available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Kitchen Sink                            3\n",
       "Healthy Sunrise                         3\n",
       "Citrus C Blend                          3\n",
       "Good Start                              3\n",
       "Individual Selections                   3\n",
       "Spicy Red with Green                    3\n",
       "Strawberry                              3\n",
       "Mango                                   3\n",
       "Tea                                     3\n",
       "Blue Green                              3\n",
       "Immune Builder                          3\n",
       "Skin Refresher                          3\n",
       "Wheat Grass                             3\n",
       "Green Basic                             3\n",
       "Energizer                               3\n",
       "Sambazon C                              3\n",
       "Green Banana Almond                     3\n",
       "Acai Berry Protein                      3\n",
       "Protein                                 3\n",
       "Avocado Toast                           2\n",
       "Tuscan Chicken                          2\n",
       "Rainbow Garden                          2\n",
       "Create-Your-Own                         2\n",
       "Green Detox                             2\n",
       "Breakfast Burrito                       2\n",
       "Harvest Kale Crunch                     2\n",
       "Roasted Chicken & Avocado               2\n",
       "Plant Burger                            2\n",
       "Dino Kale                               2\n",
       "Sambazon Bowl                           2\n",
       "                                       ..\n",
       "Espresso                                1\n",
       "Turkey Sausage & White Cheddar          1\n",
       "Pasta Choice                            1\n",
       "Sesame Soba Noodles                     1\n",
       "Drip                                    1\n",
       "Espresso Beverages                      1\n",
       "Breakfast Sandwich                      1\n",
       "Two Eggs Any Style                      1\n",
       "Soup                                    1\n",
       "Soup of the Day                         1\n",
       "Basil Pesto Tofu Scramble               1\n",
       "Peach Matcha                            1\n",
       "Body Cleanser                           1\n",
       "Pineapple Strawberry Enzyme             1\n",
       "Half Sandwich & Cup of Soup Or Salad    1\n",
       "Udon                                    1\n",
       "Nola                                    1\n",
       "Hummus Plate                            1\n",
       "Spinach & White Cheddar                 1\n",
       "Baja Fish Tacos                         1\n",
       "Miso Quinoa                             1\n",
       "Huevos Rancheros                        1\n",
       "Yellow Curry                            1\n",
       "Pacific Northwest Eggs Benedict         1\n",
       "Korean Bbq                              1\n",
       "Crushed Cucumber                        1\n",
       "Power Bowl                              1\n",
       "Wild Salmon                             1\n",
       "Beer & Wine                             1\n",
       "Sweet Greens                            1\n",
       "Name: menu_item_name, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I strongly suspect venue_name 'The Plant' is the same as 'The Plant Cafe Organic'\n",
    "blah[blah.venue_name == 'The Plant Cafe Organic'].menu_item_name.value_counts()\n",
    "blah[blah.venue_name == 'The Plant'].menu_item_name.value_counts()\n",
    "\n",
    "# But based on the items in each venue, I don't think they are the same(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Below is an alternate approach, using the Search Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # This will independently pull the venue names and id codes that correspond to the geographical areas\n",
    "# # I paired off in the previous step with grid_pairs. The names and id's will be subesquently used to\n",
    "# # construct menu url's, which I then intend to scrape.\n",
    "\n",
    "# unique_venues_from_search = []\n",
    "# unique_venue_names_from_search = []\n",
    "# start_time = time.time()\n",
    "\n",
    "# for x, y in grid_pairs:\n",
    "#     try:\n",
    "#         search = client.venues.search(params={'ll': '%.2f, %.2f' % (y, x),'query': 'food', 'limit':'50',\n",
    "#                                       'intent':'browse','radius':'800'})\n",
    "#         searched_venue_ids = [search['venues'][i]['id'] for i in range(len(search['venues']))]\n",
    "#         searched_name_ids = [search['venues'][i]['name'] for i in range(len(search['venues']))]\n",
    "#         for next_id, next_name in zip(searched_venue_ids, searched_name_ids):\n",
    "#             unique_venues_from_search.append(next_id)\n",
    "#             unique_venue_names_from_search.append(next_name)\n",
    "# #         print('--- %s loop-active seconds ---' % (time.time() - start_time))\n",
    "#     except:\n",
    "#         print('Sleeping...')\n",
    "# #         time.sleep(random.randint(115,140))\n",
    "# print('--- %s active seconds ---' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print len(unique_venue_names_from_search), len(unique_venues_from_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unique_venue_names_from_search[:5], unique_venues_from_search[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # This will only work for the Search Endpoint\n",
    "# menu_urls_from_search = []\n",
    "# base_url = 'https://foursquare.com/v/'\n",
    "# for venue_id, venue_name in zip(unique_venues_from_search, unique_venue_names_from_search):\n",
    "#     dat_id = unicodedata.normalize('NFKD', venue_id).encode('ascii','ignore')\n",
    "#     dat_name = unicodedata.normalize('NFKD', venue_name).encode('ascii','ignore')\n",
    "#     dat_name = dat_name.lower().replace('/','-').replace(' ','-')\n",
    "#     transformed_url = base_url+dat_name+'/'+dat_id+'/menu'\n",
    "#     menu_urls_from_search.append(transformed_url)\n",
    "# len(menu_urls_from_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Testing that no venues have been duplicated with my searches\n",
    "# unique_urls_from_search = list(set(menu_urls_from_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(unique_urls_from_search)\n",
    "# # This amounts to roughly 8% of my total venues searched i.e. 280/(70*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # I'd use a try except because if the manually generated urls don't yield menus,\n",
    "# # it could be because they're not a restaurant. So could be useful to later\n",
    "# # determine if they are or aren't restaurants to begin with. But for now, that goes beyond the scope of this project.\n",
    "# start_time = time.time()\n",
    "# for menu_url in unique_urls_from_search[:30]:\n",
    "#     try:\n",
    "#         parse_url(menu_url)\n",
    "#     except:\n",
    "#         pass\n",
    "#     print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# search_endpoint_df = pd.DataFrame(df_ready_rows, columns=column_headers)\n",
    "# search_endpoint_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# search_endpoint_df.loc[:50[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# search_endpoint_df.venue_menu_url.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BELOW ARE POTENTIALLY USEFUL, BUT UNUSED MATERIAL::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # This func could be useful to add the next offset to my completed venue list...\n",
    "# def extend_unique_venues(unique_venues, proposed_venue):\n",
    "#     if proposed_venue not in unique_venues:\n",
    "#         unique_venues.append(proposed_venue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Category/column titles, in order:\n",
    "# # [venue_name, venue_desc_list, venue_menu_url, venue_rated], [meta_menu_n], [depth_menus_n], [menu_item_name,\n",
    "# # menu_item_price, menu_item_desc]\n",
    "\n",
    "# venue_rows = []\n",
    "# for [venue_name, venue_desc_list, venue_menu_url, venue_rated] in venues: \n",
    "#     for meta_menu in meta_menu_n:\n",
    "#         for depth_menu in depth_menus_n:\n",
    "#             venue_rows.append([venue_name,\n",
    "#                                venue_desc_list,\n",
    "#                                venue_rated,\n",
    "#                                meta_menu,\n",
    "#                                depth_menu,\n",
    "#                                menu_item_name,\n",
    "#                                menu_item_price,\n",
    "#                                menu_item_desc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Category/column titles, in order:\n",
    "# # [venue_name, venue_desc_list, venue_menu_url, venue_rated], [meta_menu_n], [depth_menus_n], [menu_item_name,\n",
    "# # menu_item_price, menu_item_desc]\n",
    "\n",
    "# venue_dict = {}\n",
    "# for [venue_name, venue_desc_list, venue_menu_url, venue_rated] in venues:\n",
    "#     venue_dict[venue_name] = {'desc_list':venue_desc_list,\n",
    "#                               'menu_url':venue_menu_url,\n",
    "#                               'rating':venue_rated}\n",
    "    \n",
    "#     for meta_menu in meta_menu_n:\n",
    "#         venue_dict[venue_name][meta_menu] = {}\n",
    "            \n",
    "#         for depth_menu in depth_menus_n:\n",
    "#             venue_dict[venue_name][meta_menu][depth_menu] = {'menu_item_name':menu_item_name,\n",
    "#                                                              'menu_item_price':menu_item_price,\n",
    "#                                                              'menu_item_desc':menu_item_desc}\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # If for some reason I find it easier to work with category id's from API calls, I can use this block:\n",
    "# # So for instance, I can say that if pulled_id == '4bf58dd8d48988d1d3941735', append('vegan') or append(1),\n",
    "# # and that could easily serve as my target to predict on\n",
    "\n",
    "# explore = client.venues.explore(params={'ll': '%.2f, %.2f' % (37.8127675576, -122.3550796509),\n",
    "#                                         'llAcc':'100.0','radius': '6000',\n",
    "#                                         'section': 'food','limit':'50','offset':'50','sortByDistance':'1'})\n",
    "# pulled_id = explore['groups'][0]['items'][10]['venue']['categories'][0]['id']\n",
    "# print pulled_id"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
