{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Proposals:\n",
    "    \n",
    "As Americans, we have an enormous variety of foods commonly available at grocery stores, corner shops, and increasingly, online. For one project, I'd like to take a look at the fundamental ingredients and nutrients that compose this great variety. My hope would be to predict the number of nutrients found in certain food items, as well as looking hollistically at the ingredients that are most common in our foods. I would like to then relate this to general dietary habits of the US population, and get a granular idea of the nutrient composition of diets. The stretch goal would be to predict the changes in nutrient-use given an increasing population of vegans, where I could estimate increases in required nutrient needs based on 'normal' diets. I'm unsure whether this would be kosher, because obviously I can't check the quality of my predictions for the future. I could, however, see if I could predict correlations on historical data in growth of the vegan diet in the US, and then use that model to predict the effect of future growth of vegan diets.\n",
    "       \n",
    "*** (I'm still pondering the best way to go about this, but at least you can see where I'm hoping to take this) ***\n",
    "    \n",
    "'Nutrient Database from 2012' - https://catalog.data.gov/dataset/usda-national-nutrient-database-for-standard-reference\n",
    "'Nutrient Database from 2009' - https://catalog.data.gov/dataset/usda-national-nutrient-database-for-standard-reference-release-22\n",
    "    \n",
    "Partially pre-cleaned nutrient data - https://github.com/mhess126/usda_national_nutrients\n",
    "USDA API (not sure if this could be useful yet?) - https://ndb.nal.usda.gov/ndb/api/doc\n",
    "    \n",
    "    \n",
    "I'm still looking for better data to work with that could give me some idea of dietary habits, but here's where I've been looking - https://catalog.data.gov/dataset?q=bureauCode:%22005:13%22 ; https://catalog.data.gov/dataset?q=usda+consumption+national+nutrient&sort=views_recent+desc&ext_location=&ext_bbox=&ext_prev_extent=-142.03125%2C2.4601811810210052%2C-59.0625%2C58.63121664342478\n",
    "    \n",
    "    \n",
    "Alternatively, I could look at pricing these ingredients, based on the foodtypes that we find them in. For example, take a chili sauce. Of this sauce, take a look at the unique ingredients, and their respective portion size in the sauce. Let's say that black beans compose 20% of the chili, and the chili runs 5 dollars/unit. Then the pricing for the black bean ingredient would be 1 dollar.\n",
    "\n",
    "From there, I would want to observe how expensive these ingredients can get and how their prices change depending on what products they may be found in. \n",
    "    \n",
    "'Food Price Outlook, current' - https://catalog.data.gov/dataset/food-price-outlook\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pip install scrapy\n",
    "# pip install --upgrade zope2\n",
    "\n",
    "import foursquare\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "from scrapy import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "import seaborn as sns\n",
    "import time\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CLIENT_ID = '33NDJLQ342FAMTNX5Z55PR0PQQOJZRAZZ3XEAI0ERQXEJRUL'\n",
    "CLIENT_SECRET = 'FGMFNZGMWUR1ILZFGH2NV1OKQY3WK5AAPHWKXTFWRR3B4Z4E'\n",
    "client = foursquare.Foursquare(client_id=CLIENT_ID, client_secret=CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-122.355079651\n",
      "[37.8127675576, 37.7078622611]\n",
      "360\n"
     ]
    }
   ],
   "source": [
    "#Let's define a geo dictionary whose bounds encompass all of SF, and another 4 miles south as well (so ~7mi x 11mi)\n",
    "ne = {'ne_lat': -122.3550796509, 'ne_long': 37.8127675576}\n",
    "sw = {'sw_lat': -122.5164413452, 'sw_long': 37.7078622611}\n",
    "\n",
    "# east/west\n",
    "lat_bounds = [ne['ne_lat'], sw['sw_lat']]\n",
    "print lat_bounds[0]\n",
    "# north/south\n",
    "lon_bounds = [ne['ne_long'], sw['sw_long']]\n",
    "print lon_bounds\n",
    "\n",
    "#increment ~ half a mile (in latitude/longitude), is 0.007\n",
    "increment = 0.007\n",
    "\n",
    "# The gridding below moves North, starting from the bottom SW corner boundary, and then moves east half a mile,\n",
    "# and repeats the process until stopping at the NE corner boundary.\n",
    "grid_pairs = []\n",
    "for lat in np.arange(lat_bounds[1], lat_bounds[0], increment):\n",
    "    for lon in np.arange(lon_bounds[1], lon_bounds[0], increment):\n",
    "        grid_pairs.append([lat, lon])\n",
    "        \n",
    "print len(grid_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTES:\n",
    "# should i make the loop above this cell more efficient by checking and storing which url's don't work?\n",
    "# B/C then I can exclude them the next time i run my loops ; BUT, it's probably better to keep track of these,\n",
    "# because while some won't even be eateries, many will be eateries that simply don't have foursqare menus.\n",
    "\n",
    "# In such cases, knowing the venue information could still be valuable, because we can surface those to users who\n",
    "# wish to manually add items (could possibly add items by taking pictures of the menu where the item is located)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here, I work with the Explore Endpoint\n",
    "\n",
    "# This will pull the venue names and url's (labeled as 'explored_venue_ids') that correspond to the geographical areas\n",
    "# I paired off in the previous step with grid_pairs. These are the  urls that I then intend to scrape in subsequent\n",
    "# steps.\n",
    "\n",
    "unique_venues_from_explore = []\n",
    "unique_venue_names_from_explore = []\n",
    "start_time = time.time()\n",
    "counter = 0\n",
    "for x, y in grid_pairs:\n",
    "    for offset in range(50, 251, 50):\n",
    "        try:\n",
    "            # Radius is radius in meters around given 'll'; 800 meters\n",
    "            # is approx 0.5 miles (but I may adjust the radius going forward)\n",
    "        \n",
    "            explore = client.venues.explore(params={'ll': '%.2f, %.2f' % (y, x), 'llAcc':'100.0','radius': '2000',\n",
    "                                               'section': 'food','limit':'50','offset':''+str(offset)+'',\n",
    "                                                    'sortByDistance':'1'})\n",
    "            explored_venue_ids = []\n",
    "            explored_venue_names = []\n",
    "\n",
    "            for i in range(len(explore['groups'][0]['items'])):\n",
    "                try:\n",
    "                    pulled_id = explore['groups'][0]['items'][i]['venue']['menu']['url']\n",
    "                    explored_venue_ids.append(pulled_id)\n",
    "                    pulled_name = explore['groups'][0]['items'][i]['venue']['name']\n",
    "                    explored_venue_names.append(pulled_name)\n",
    "                except:\n",
    "                    pass\n",
    "            for next_id, next_name in zip(explored_venue_ids, explored_venue_names):\n",
    "                if 'foursquare.com' in str(next_id):\n",
    "                    unique_venues_from_explore.append(next_id)\n",
    "                    unique_venue_names_from_explore.append(next_name)\n",
    "            counter += 1\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time)), \"loop number:\", counter\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(unique_venue_names_from_explore), len(unique_venues_from_explore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_venues_from_explore = list(set(unique_venues_from_explore))\n",
    "unique_venue_names_from_explore = list(set(unique_venue_names_from_explore))\n",
    "unique_but_chained_venues = len(unique_venues_from_explore) - len(unique_venue_names_from_explore)\n",
    "\n",
    "print len(unique_venues_from_explore), unique_but_chained_venues\n",
    "# So ~ 18% of our results are unique venues (and another 48 of them are unique venus, but w/ the same name i.e. chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now that I have plenty of urls, let's plug them into a scraper so I can populate my df. Here are the headers\n",
    "# I'm looking to get as well...\n",
    "\n",
    "column_headers = ['venue_name', 'venue_desc_list', 'vegan_venue_check', 'venue_menu_url', 'venue_rated', 'meta_menu_n', 'depth_menus_n',\n",
    "                  'menu_item_name', 'menu_item_price', 'menu_item_desc']\n",
    "\n",
    "df_ready_rows = []\n",
    "def parse_url(url, data=False):\n",
    "\n",
    "    response               =  requests.get(url)\n",
    "    \n",
    "    #Steps:\n",
    "    #1) get the unicode objects\n",
    "    #2) change objects from unicode to string\n",
    "    \n",
    "    venue_name_uni         = Selector(text=response.text).xpath('//h1[@class=\"venueName\"]/text()').extract()\n",
    "    venue_name             = unicodedata.normalize('NFKD', venue_name_uni[0]).encode('ascii','ignore')\n",
    "    \n",
    "    # I also need to do an iteration to capture the multiple descriptors.\n",
    "    # I'll store the descriptors in a list to capture the entire description:\n",
    "    venue_desc_uni         =  Selector(text=response.text).xpath('//span[@class=\"unlinkedCategory\"]/\\\n",
    "    text()').extract()\n",
    "    venue_desc_list        = []\n",
    "    vegan_venue_check       = [] # I'll also do a quick check to see up front if we have a vegan menu on our hands\n",
    "    for venue_desc_phrase in range(len(venue_desc_uni)):\n",
    "        venue_desc_n       = unicodedata.normalize('NFKD', venue_desc_uni[venue_desc_phrase]).encode('ascii',\n",
    "                                                                                                  'ignore')\n",
    "        venue_desc_list.append(venue_desc_n)\n",
    "\n",
    "    flat_desc = ' '.join(venue_desc_list)\n",
    "    if 'vegetarian / vegan restaurant' in flat_desc.lower():\n",
    "        vegan_venue_check = 'vegan'\n",
    "    else:\n",
    "        vegan_venue_check = 'not_vegan'\n",
    "\n",
    "    # The url too, right?\n",
    "    venue_menu_url         = url\n",
    "\n",
    "    # Grabbing the venue rating, just a note: venueScore positive/neutral/negative, but I'm only getting the\n",
    "    # rating number, on a scale 1-10\n",
    "    venueScore_options     = ['positive','neutral','negative']\n",
    "    venue_rated = []\n",
    "    for vs_option in venueScore_options:\n",
    "        try:\n",
    "            venue_rating_uni        = Selector(text=response.text).xpath('//div[@class=\"venueRateBlock  \"]/\\\n",
    "    span[@class=\"venueScore '+vs_option+'\"]/span/text()').extract()\n",
    "            venue_rated             = unicodedata.normalize('NFKD', venue_rating_uni[0]).encode('ascii','ignore')\n",
    "            venue_rated = float(venue_rated)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # Even if there is no rating, I'd still like to keep track of that...\n",
    "    if venue_rated == np.nan:\n",
    "        venue_rated = 'rating_not_available'\n",
    "        \n",
    "    #And I'll transform the list back into a string...\n",
    "#     venue_rated = venue_rated[0]\n",
    "    \n",
    "    #NOTE: do i also need to account for when menus don't have titles? because in that case meta_menu_list\n",
    "    #could/would\n",
    "    #return null. if so, perhaps just do a 'try excepct:pass' function if it can't find titles, but could it still\n",
    "    #grab the menu items? maybe i should just put in a \"null title\" for the meta_menu_n to overcome this\n",
    "    #I no longer think this is an issue, but maybe something to put in the appendix for later?\n",
    "    \n",
    "    meta_menu_list      =  Selector(text=response.text).xpath('//h2[@class=\"categoryName\"]/text()').extract()\n",
    "        \n",
    "    for meta_menu_item in range(len(meta_menu_list)):\n",
    "        \n",
    "        meta_menu_n         = unicodedata.normalize('NFKD', meta_menu_list[meta_menu_item]).encode('ascii',\n",
    "                                                                                                   'ignore')\n",
    "        \n",
    "#         print \"meta menu title %d:\" %(meta_menu_item+1), meta_menu_n, \"# of meta menus:\", len(meta_menu_list)\n",
    "        \n",
    "        depth_menus_n_uni = Selector(text=response.text).xpath('//div[@class=\"menu\"]['+str(meta_menu_item+1)+']/\\\n",
    "        div[@class=\"menuItems\"]/div[@class=\"section\"]/div[@class=\"sectionHeader\"]/\\\n",
    "        div[@class=\"sectionName\"]/text()').extract()\n",
    "        \n",
    "        for meta_depth_nn in range(len(depth_menus_n_uni)):\n",
    "            \n",
    "            depth_menus_n     = unicodedata.normalize('NFKD', depth_menus_n_uni[meta_depth_nn]\n",
    "                                                     ).encode('ascii','ignore')\n",
    "\n",
    "            #get the name of the depth menu, and record it's location as 'n_level'\n",
    "            n_level = meta_depth_nn+1\n",
    "#             print \"depth menu title %d:\" %(n_level), depth_menus_n\n",
    "            \n",
    "            #let's grab the entire depth menu:\n",
    "            depth_menu_id_uni = Selector(text=response.text).xpath('//div[@class=\"menu\"]\\\n",
    "            ['+str(meta_menu_item+1)+']/div[@class=\"menuItems\"]/div[@class=\"section\"]['+str(n_level)+']/\\\n",
    "            div[@class=\"sectionHeader\"]/div[@class=\"sectionName\"]/text()').extract()\n",
    "            depth_menu_id     = unicodedata.normalize('NFKD', depth_menu_id_uni[0]\n",
    "                                                     ).encode('ascii','ignore')\n",
    "            depth_menu_id = len(depth_menu_id_uni)\n",
    "#             print \"#id of depth menu:\", depth_menu_id\n",
    "            \n",
    "            #loop throught the left and right side of each container:\n",
    "            left_or_right_list = ['left','right']\n",
    "            \n",
    "            for left_or_right in left_or_right_list:\n",
    "    \n",
    "                #need the length of the [left/right] container, to iterate through:\n",
    "                container_len_uni = Selector(text=response.text).xpath('//div[@class=\"menu\"]\\\n",
    "                ['+str(meta_menu_item+1)+']/div[@class=\"menuItems\"]/div[@class=\"section\"]['+str(n_level)+']/div\\\n",
    "                [@class=\"entryContainer\"]/div[@class=\"'+left_or_right+'Column\"]/\\\n",
    "                div[@class=\"entry\"]/node()[1]//text()').extract()\n",
    "#                 print \"left_check:\", left_or_right, \"contain len:\", len(container_len_uni)\n",
    "            \n",
    "                for section_n in range(len(container_len_uni)):                    \n",
    "                    \n",
    "                    #now we can get the name of that menu item...\n",
    "                    menu_item_name_uni = Selector(text=response.text).xpath('//div[@class=\"menu\"]\\\n",
    "                    ['+str(meta_menu_item+1)+']/div[@class=\"menuItems\"]/div[@class=\"section\"]\\\n",
    "                    ['+str(n_level)+']/div\\\n",
    "                    [@class=\"entryContainer\"]/div[@class=\"'+left_or_right+'Column\"]/div[@class=\"entry\"]\\\n",
    "                    ['+str(section_n+1)+']/node()[1]//text()').extract()\n",
    "                    menu_item_name     = unicodedata.normalize('NFKD', menu_item_name_uni[0]\n",
    "                                                                    ).encode('ascii','ignore')\n",
    "#                     print \"menu_item_name:\", menu_item_name\n",
    "                    \n",
    "                    #and then we can get the price (if there is one...)\n",
    "                    try:\n",
    "                        menu_item_price_uni = Selector(text=response.text).xpath('//div[@class=\"menu\"]\\\n",
    "                    ['+str(meta_menu_item+1)+']/div[@class=\"menuItems\"]/div[@class=\"section\"]\\\n",
    "                    ['+str(n_level)+']/div\\\n",
    "                    [@class=\"entryContainer\"]/div[@class=\"'+left_or_right+'Column\"]/div[@class=\"entry\"]\\\n",
    "                    ['+str(section_n+1)+']/node()[2]//text()').extract()\n",
    "                        menu_item_price     = unicodedata.normalize('NFKD', menu_item_price_uni[0]\n",
    "                                                                    ).encode('ascii','ignore')\n",
    "                        menu_item_price = float(menu_item_price)\n",
    "#                         print \"menu_item_price:\", menu_item_price\n",
    "                    except:\n",
    "#                         print \"menu_item_price:\", \"price_not_available\"\n",
    "                        menu_item_price = 'price_not_available'\n",
    "                    \n",
    "                    #and finally the description (if there is one...)\n",
    "                    try:\n",
    "                        menu_item_desc_uni = Selector(text=response.text).xpath('//div[@class=\"menu\"]\\\n",
    "                    ['+str(meta_menu_item+1)+']/div[@class=\"menuItems\"]/div[@class=\"section\"]\\\n",
    "                    ['+str(n_level)+']/div\\\n",
    "                    [@class=\"entryContainer\"]/div[@class=\"'+left_or_right+'Column\"]/div[@class=\"entry\"]\\\n",
    "                    ['+str(section_n+1)+']/node()[3]//text()').extract()\n",
    "                        menu_item_desc     = unicodedata.normalize('NFKD', menu_item_desc_uni[0]\n",
    "                                                                    ).encode('ascii','ignore')\n",
    "#                         print \"menu_item_desc:\", menu_item_desc\n",
    "                    except:\n",
    "#                         print \"menu_item_desc:\", \"desc_not_available\"\n",
    "                        menu_item_desc = 'desc_not_available'\n",
    "\n",
    "                    # Finally, I'll append my results so that when I wrap up the fuction, I can finish with\n",
    "                    # a prepared set of info, dataframe ready.\n",
    "                    df_ready_rows.append([venue_name,\n",
    "                                          venue_desc_list,\n",
    "                                          vegan_venue_check,\n",
    "                                          venue_menu_url,\n",
    "                                          venue_rated,\n",
    "                                          meta_menu_n,\n",
    "                                          depth_menus_n,\n",
    "                                          menu_item_name,\n",
    "                                          menu_item_price,\n",
    "                                          menu_item_desc])\n",
    "    return df_ready_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Actually, through the Explore endpoint, I was able to directly grab the menu url, so no need to manually build my\n",
    "# url this time...\n",
    "start_time = time.time()\n",
    "countered = 0\n",
    "for menu_url in unique_venues_from_explore:\n",
    "    try:\n",
    "        parse_url(menu_url)\n",
    "    except:\n",
    "        pass\n",
    "    countered += 1\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time)), \"loop number:\", countered\n",
    "#Takes about 4 mins for 30 url's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(df_ready_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explore_endpoint_df = pd.DataFrame(df_ready_rows, columns=column_headers)\n",
    "explore_endpoint_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # I ran my script previously, and just saved a local copy...\n",
    "# explore_endpoint_df.to_pickle('../../projects/Capstone Stuff/explore_endpoint_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explore_endpoint_df = pd.read_pickle('../../projects/Capstone Stuff/explore_endpoint_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "926"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(explore_endpoint_df.venue_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "not_vegan    88279\n",
       "vegan         1861\n",
       "Name: vegan_venue_check, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore_endpoint_df.vegan_venue_check.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>venue_name</th>\n",
       "      <th>venue_desc_list</th>\n",
       "      <th>vegan_venue_check</th>\n",
       "      <th>venue_menu_url</th>\n",
       "      <th>venue_rated</th>\n",
       "      <th>meta_menu_n</th>\n",
       "      <th>depth_menus_n</th>\n",
       "      <th>menu_item_name</th>\n",
       "      <th>menu_item_price</th>\n",
       "      <th>menu_item_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hunan Chef</td>\n",
       "      <td>[Hunan Restaurant]</td>\n",
       "      <td>not_vegan</td>\n",
       "      <td>https://foursquare.com/v/hunan-chef/4bcfa515a8...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>Rice Plates</td>\n",
       "      <td>Beef Or Chicken with Broccoli Over Ricea</td>\n",
       "      <td>5.25</td>\n",
       "      <td>desc_not_available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hunan Chef</td>\n",
       "      <td>[Hunan Restaurant]</td>\n",
       "      <td>not_vegan</td>\n",
       "      <td>https://foursquare.com/v/hunan-chef/4bcfa515a8...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>Rice Plates</td>\n",
       "      <td>Beef Or Chicken with Fresh Mushroom in Oyster ...</td>\n",
       "      <td>5.25</td>\n",
       "      <td>desc_not_available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hunan Chef</td>\n",
       "      <td>[Hunan Restaurant]</td>\n",
       "      <td>not_vegan</td>\n",
       "      <td>https://foursquare.com/v/hunan-chef/4bcfa515a8...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>Rice Plates</td>\n",
       "      <td>Beef with Mixed Vegetables Over Ricea</td>\n",
       "      <td>5.25</td>\n",
       "      <td>desc_not_available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hunan Chef</td>\n",
       "      <td>[Hunan Restaurant]</td>\n",
       "      <td>not_vegan</td>\n",
       "      <td>https://foursquare.com/v/hunan-chef/4bcfa515a8...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>Rice Plates</td>\n",
       "      <td>Beef with Scrambled Eggs Over Ricea</td>\n",
       "      <td>5.25</td>\n",
       "      <td>desc_not_available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hunan Chef</td>\n",
       "      <td>[Hunan Restaurant]</td>\n",
       "      <td>not_vegan</td>\n",
       "      <td>https://foursquare.com/v/hunan-chef/4bcfa515a8...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>Rice Plates</td>\n",
       "      <td>Roasted Duck Over Ricea</td>\n",
       "      <td>5.75</td>\n",
       "      <td>desc_not_available</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   venue_name     venue_desc_list vegan_venue_check  \\\n",
       "0  Hunan Chef  [Hunan Restaurant]         not_vegan   \n",
       "1  Hunan Chef  [Hunan Restaurant]         not_vegan   \n",
       "2  Hunan Chef  [Hunan Restaurant]         not_vegan   \n",
       "3  Hunan Chef  [Hunan Restaurant]         not_vegan   \n",
       "4  Hunan Chef  [Hunan Restaurant]         not_vegan   \n",
       "\n",
       "                                      venue_menu_url venue_rated meta_menu_n  \\\n",
       "0  https://foursquare.com/v/hunan-chef/4bcfa515a8...         7.3       Lunch   \n",
       "1  https://foursquare.com/v/hunan-chef/4bcfa515a8...         7.3       Lunch   \n",
       "2  https://foursquare.com/v/hunan-chef/4bcfa515a8...         7.3       Lunch   \n",
       "3  https://foursquare.com/v/hunan-chef/4bcfa515a8...         7.3       Lunch   \n",
       "4  https://foursquare.com/v/hunan-chef/4bcfa515a8...         7.3       Lunch   \n",
       "\n",
       "  depth_menus_n                                     menu_item_name  \\\n",
       "0   Rice Plates           Beef Or Chicken with Broccoli Over Ricea   \n",
       "1   Rice Plates  Beef Or Chicken with Fresh Mushroom in Oyster ...   \n",
       "2   Rice Plates              Beef with Mixed Vegetables Over Ricea   \n",
       "3   Rice Plates                Beef with Scrambled Eggs Over Ricea   \n",
       "4   Rice Plates                            Roasted Duck Over Ricea   \n",
       "\n",
       "  menu_item_price      menu_item_desc  \n",
       "0            5.25  desc_not_available  \n",
       "1            5.25  desc_not_available  \n",
       "2            5.25  desc_not_available  \n",
       "3            5.25  desc_not_available  \n",
       "4            5.75  desc_not_available  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore_endpoint_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some tom-foolery w/ regards to the explore_endpoint_df is below, before my final notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here I'm just testing out a prototype search function\n",
    "count = 0\n",
    "loc_list = []\n",
    "for i in range(explore_endpoint_df.shape[0]):\n",
    "    flat_desc = ' '.join(explore_endpoint_df.venue_desc_list[i])\n",
    "    if 'american' in flat_desc.lower():\n",
    "        count += 1\n",
    "        loc_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>venue_name</th>\n",
       "      <th>venue_desc_list</th>\n",
       "      <th>vegan_venue_check</th>\n",
       "      <th>venue_menu_url</th>\n",
       "      <th>venue_rated</th>\n",
       "      <th>meta_menu_n</th>\n",
       "      <th>depth_menus_n</th>\n",
       "      <th>menu_item_name</th>\n",
       "      <th>menu_item_price</th>\n",
       "      <th>menu_item_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90134</th>\n",
       "      <td>The Corner Store</td>\n",
       "      <td>[American Restaurant, Cocktail Bar, Bar]</td>\n",
       "      <td>not_vegan</td>\n",
       "      <td>https://foursquare.com/v/the-corner-store/502e...</td>\n",
       "      <td>8.4</td>\n",
       "      <td>Brunch Menu</td>\n",
       "      <td>Sweet Stuff</td>\n",
       "      <td>Strauss Farm Soft Serve</td>\n",
       "      <td>6</td>\n",
       "      <td>Vanilla, Chocolate or Twist. Toppings: Oreo, B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             venue_name                           venue_desc_list  \\\n",
       "90134  The Corner Store  [American Restaurant, Cocktail Bar, Bar]   \n",
       "\n",
       "      vegan_venue_check                                     venue_menu_url  \\\n",
       "90134         not_vegan  https://foursquare.com/v/the-corner-store/502e...   \n",
       "\n",
       "      venue_rated  meta_menu_n depth_menus_n           menu_item_name  \\\n",
       "90134         8.4  Brunch Menu   Sweet Stuff  Strauss Farm Soft Serve   \n",
       "\n",
       "      menu_item_price                                     menu_item_desc  \n",
       "90134               6  Vanilla, Chocolate or Twist. Toppings: Oreo, B...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in loc_list:\n",
    "    dffff = explore_endpoint_df.loc[[i]]\n",
    "dffff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Below is an alternate approach, using the Search Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will independently pull the venue names and id codes that correspond to the geographical areas\n",
    "# I paired off in the previous step with grid_pairs. The names and id's will be subesquently used to\n",
    "# construct menu url's, which I then intend to scrape.\n",
    "\n",
    "unique_venues_from_search = []\n",
    "unique_venue_names_from_search = []\n",
    "start_time = time.time()\n",
    "\n",
    "for x, y in grid_pairs:\n",
    "    try:\n",
    "        search = client.venues.search(params={'ll': '%.2f, %.2f' % (y, x),'query': 'food', 'limit':'50',\n",
    "                                      'intent':'browse','radius':'800'})\n",
    "        searched_venue_ids = [search['venues'][i]['id'] for i in range(len(search['venues']))]\n",
    "        searched_name_ids = [search['venues'][i]['name'] for i in range(len(search['venues']))]\n",
    "        for next_id, next_name in zip(searched_venue_ids, searched_name_ids):\n",
    "            unique_venues_from_search.append(next_id)\n",
    "            unique_venue_names_from_search.append(next_name)\n",
    "#         print('--- %s loop-active seconds ---' % (time.time() - start_time))\n",
    "    except:\n",
    "        print('Sleeping...')\n",
    "#         time.sleep(random.randint(115,140))\n",
    "print('--- %s active seconds ---' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(unique_venue_names_from_search), len(unique_venues_from_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_venue_names_from_search[:5], unique_venues_from_search[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will only work for the Search Endpoint\n",
    "menu_urls_from_search = []\n",
    "base_url = 'https://foursquare.com/v/'\n",
    "for venue_id, venue_name in zip(unique_venues_from_search, unique_venue_names_from_search):\n",
    "    dat_id = unicodedata.normalize('NFKD', venue_id).encode('ascii','ignore')\n",
    "    dat_name = unicodedata.normalize('NFKD', venue_name).encode('ascii','ignore')\n",
    "    dat_name = dat_name.lower().replace('/','-').replace(' ','-')\n",
    "    transformed_url = base_url+dat_name+'/'+dat_id+'/menu'\n",
    "    menu_urls_from_search.append(transformed_url)\n",
    "len(menu_urls_from_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing that no venues have been duplicated with my searches\n",
    "unique_urls_from_search = list(set(menu_urls_from_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(unique_urls_from_search)\n",
    "#This amounts to roughly 8% of my total venues searched i.e. 280/(70*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I'd use a try except because if the manually generated urls don't yield menus,\n",
    "# it could be because they're not a restaurant. So could be useful to later\n",
    "# determine if they are or aren't restaurants to begin with. But for now, that goes beyond the scope of this project.\n",
    "start_time = time.time()\n",
    "for menu_url in unique_urls_from_search[:30]:\n",
    "    try:\n",
    "        parse_url(menu_url)\n",
    "    except:\n",
    "        pass\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_endpoint_df = pd.DataFrame(df_ready_rows, columns=column_headers)\n",
    "search_endpoint_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_endpoint_df.loc[:50[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_endpoint_df.venue_menu_url.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BELOW ARE POTENTIALLY USEFUL, BUT UNUSED MATERIAL::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This func could be useful to add the next offset to my completed venue list...\n",
    "def extend_unique_venues(unique_venues, proposed_venue):\n",
    "    if proposed_venue not in unique_venues:\n",
    "        unique_venues.append(proposed_venue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Category/column titles, in order:\n",
    "# # [venue_name, venue_desc_list, venue_menu_url, venue_rated], [meta_menu_n], [depth_menus_n], [menu_item_name,\n",
    "# # menu_item_price, menu_item_desc]\n",
    "\n",
    "# venue_rows = []\n",
    "# for [venue_name, venue_desc_list, venue_menu_url, venue_rated] in venues: \n",
    "#     for meta_menu in meta_menu_n:\n",
    "#         for depth_menu in depth_menus_n:\n",
    "#             venue_rows.append([venue_name,\n",
    "#                                venue_desc_list,\n",
    "#                                venue_rated,\n",
    "#                                meta_menu,\n",
    "#                                depth_menu,\n",
    "#                                menu_item_name,\n",
    "#                                menu_item_price,\n",
    "#                                menu_item_desc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Category/column titles, in order:\n",
    "# # [venue_name, venue_desc_list, venue_menu_url, venue_rated], [meta_menu_n], [depth_menus_n], [menu_item_name,\n",
    "# # menu_item_price, menu_item_desc]\n",
    "\n",
    "# venue_dict = {}\n",
    "# for [venue_name, venue_desc_list, venue_menu_url, venue_rated] in venues:\n",
    "#     venue_dict[venue_name] = {'desc_list':venue_desc_list,\n",
    "#                               'menu_url':venue_menu_url,\n",
    "#                               'rating':venue_rated}\n",
    "    \n",
    "#     for meta_menu in meta_menu_n:\n",
    "#         venue_dict[venue_name][meta_menu] = {}\n",
    "            \n",
    "#         for depth_menu in depth_menus_n:\n",
    "#             venue_dict[venue_name][meta_menu][depth_menu] = {'menu_item_name':menu_item_name,\n",
    "#                                                              'menu_item_price':menu_item_price,\n",
    "#                                                              'menu_item_desc':menu_item_desc}\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # If for some reason I find it easier to work with category id's from API calls, I can use this block:\n",
    "# # So for instance, I can say that if pulled_id == '4bf58dd8d48988d1d3941735', append('vegan') or append(1),\n",
    "# # and that could easily serve as my target to predict on\n",
    "\n",
    "# explore = client.venues.explore(params={'ll': '%.2f, %.2f' % (37.8127675576, -122.3550796509),\n",
    "#                                         'llAcc':'100.0','radius': '6000',\n",
    "#                                         'section': 'food','limit':'50','offset':'50','sortByDistance':'1'})\n",
    "# pulled_id = explore['groups'][0]['items'][10]['venue']['categories'][0]['id']\n",
    "# print pulled_id"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
